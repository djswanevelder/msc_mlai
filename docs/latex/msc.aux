\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\babel@aux[2]{}
\@nameuse{bbl@beforestart}
\catcode `"\active 
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\bibstyle{IEEEtran}
\babel@aux{english}{}
\pgfsyspdfmark {pgfid1}{4661699}{49769019}
\@writefile{toc}{\contentsline {chapter}{Abstract}{i}{chapter*.1}\protected@file@percent }
\citation{Dean2017BlackBox}
\citation{huggingface2024review}
\citation{schurholt2022modelzoosdatasetdiverse}
\citation{unterthiner2021predictingneuralnetworkaccuracy}
\citation{salama2024datasetsizerecoverylora}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Introduction}{1}{chapter.1}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{chap:introduction}{{1}{1}{Introduction}{chapter.1}{}}
\citation{schurholt2022hyperrepresentationsgenerativemodelssampling}
\citation{pmlr-v235-schurholt24a}
\citation{bedionita2025instructionguidedautoregressiveneuralnetwork}
\citation{meynent2025structureenoughleveragingbehavior}
\citation{radford2021learningtransferablevisualmodels}
\citation{agren2022ntxentlossupperbound}
\@writefile{lof}{\contentsline {figure}{\numberline {1.1}{\ignorespaces A Conditonal Model Sampling system, embedding a dataset, model weights and results into a shared embedding space.}}{3}{figure.caption.3}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:pipeline}{{1.1}{3}{A Conditonal Model Sampling system, embedding a dataset, model weights and results into a shared embedding space}{figure.caption.3}{}}
\citation{}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Background}{4}{chapter.2}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{chap:background}{{2}{4}{Background}{chapter.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Weight Space Learning}{4}{section.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Principal Component Analysis (PCA)}{5}{section.2.2}\protected@file@percent }
\newlabel{sec:pca}{{2.2}{5}{Principal Component Analysis (PCA)}{section.2.2}{}}
\newlabel{eq:pca}{{2.1}{5}{Principal Component Analysis (PCA)}{equation.2.2.1}{}}
\newlabel{eq:pca_reconstruction}{{2.2}{6}{Principal Component Analysis (PCA)}{equation.2.2.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.3}Autoencoders}{6}{section.2.3}\protected@file@percent }
\newlabel{sec:autoencoders}{{2.3}{6}{Autoencoders}{section.2.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces Autoencoder encoding input data into a latent representation and decoding to reconstructed data}}{6}{figure.caption.4}\protected@file@percent }
\newlabel{fig:autoencoder}{{2.1}{6}{Autoencoder encoding input data into a latent representation and decoding to reconstructed data}{figure.caption.4}{}}
\citation{}
\citation{}
\newlabel{eq:encoder}{{2.3}{7}{Autoencoders}{equation.2.3.3}{}}
\newlabel{eq:decoder}{{2.4}{7}{Autoencoders}{equation.2.3.4}{}}
\newlabel{eq:composite}{{2.5}{7}{Autoencoders}{equation.2.3.5}{}}
\newlabel{eq:ae_mse_loss}{{2.6}{7}{Autoencoders}{equation.2.3.6}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.4}Contrastive Learning}{7}{section.2.4}\protected@file@percent }
\newlabel{sec:contrastive}{{2.4}{7}{Contrastive Learning}{section.2.4}{}}
\citation{}
\@writefile{lof}{\contentsline {figure}{\numberline {2.2}{\ignorespaces Contrastive encoder pulling similiar data pairs $(x,x^+)$ together in latent space, and pushing dissimilar pairs $(x,x^-)$ apart}}{8}{figure.caption.5}\protected@file@percent }
\newlabel{fig:contrastive}{{2.2}{8}{Contrastive encoder pulling similiar data pairs $(x,x^+)$ together in latent space, and pushing dissimilar pairs $(x,x^-)$ apart}{figure.caption.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.1}NT-Xent Loss}{8}{subsection.2.4.1}\protected@file@percent }
\newlabel{sec:ntxne_loss}{{2.4.1}{8}{NT-Xent Loss}{subsection.2.4.1}{}}
\newlabel{eq:nt-xent_loss}{{2.7}{8}{NT-Xent Loss}{equation.2.4.7}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.2}Multimodal Embedding}{8}{subsection.2.4.2}\protected@file@percent }
\citation{NEURIPS2022_b2c4b7d3}
\@writefile{toc}{\contentsline {section}{\numberline {2.5}Autoencoder for Neural Embeddings}{9}{section.2.5}\protected@file@percent }
\newlabel{eq:multi_loss}{{2.8}{9}{Autoencoder for Neural Embeddings}{equation.2.5.8}{}}
\citation{pmlr-v235-schurholt24a}
\citation{pmlr-v235-schurholt24a}
\citation{pmlr-v235-schurholt24a}
\@writefile{lof}{\contentsline {figure}{\numberline {2.3}{\ignorespaces Sequential Autoencoder for Neural Embeddings (SANE) generating a cloud of embeddings for a model weight ~\cite  {pmlr-v235-schurholt24a}}}{10}{figure.caption.6}\protected@file@percent }
\newlabel{fig:sane}{{2.3}{10}{Sequential Autoencoder for Neural Embeddings (SANE) generating a cloud of embeddings for a model weight ~\cite {pmlr-v235-schurholt24a}}{figure.caption.6}{}}
\newlabel{eq:weight_recon_loss}{{2.9}{10}{Autoencoder for Neural Embeddings}{equation.2.5.9}{}}
\citation{pmlr-v235-schurholt24a}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Methodology}{12}{chapter.3}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{chap:method}{{3}{12}{Methodology}{chapter.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.1}{\ignorespaces The process of embedding a dataset, model weigths and results into a shared embedding space }}{13}{figure.caption.7}\protected@file@percent }
\newlabel{fig:pipeline_2}{{3.1}{13}{The process of embedding a dataset, model weigths and results into a shared embedding space}{figure.caption.7}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Model Zoo Generation}{13}{section.3.1}\protected@file@percent }
\newlabel{sec:data_gen}{{3.1}{13}{Model Zoo Generation}{section.3.1}{}}
\@writefile{lot}{\contentsline {table}{\numberline {3.1}{\ignorespaces Model Zoo Hyperparameter Sampling Distributions and Fixed Parameters\relax }}{14}{table.caption.8}\protected@file@percent }
\newlabel{tbl:model_zoo_params}{{3.1}{14}{Model Zoo Hyperparameter Sampling Distributions and Fixed Parameters\relax }{table.caption.8}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.2}Model}{14}{section.3.2}\protected@file@percent }
\newlabel{sec:model}{{3.2}{14}{Model}{section.3.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.1}Weight Encoder}{14}{subsection.3.2.1}\protected@file@percent }
\newlabel{subsec:weight_enc}{{3.2.1}{14}{Weight Encoder}{subsection.3.2.1}{}}
\citation{some_reference}
\@writefile{loa}{\contentsline {algorithm}{\numberline {3.1}{\ignorespaces Mode Selection During PCA Fitting\relax }}{15}{algorithm.3.1}\protected@file@percent }
\newlabel{alg:pca-mode-selection}{{3.1}{15}{Mode Selection During PCA Fitting\relax }{algorithm.3.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.2}{\ignorespaces Two-stage compression of model weights: PCA reduces each parameter tensor, followed by an autoencoder producing a compact latent representation.\relax }}{16}{figure.caption.9}\protected@file@percent }
\newlabel{fig:pca_ae}{{3.2}{16}{Two-stage compression of model weights: PCA reduces each parameter tensor, followed by an autoencoder producing a compact latent representation.\relax }{figure.caption.9}{}}
\newlabel{eq:pca_ae_loss}{{3.1}{16}{Weight Encoder}{equation.3.2.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.2}Dataset Encoder}{17}{subsection.3.2.2}\protected@file@percent }
\newlabel{subsec:data_enc}{{3.2.2}{17}{Dataset Encoder}{subsection.3.2.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.3}Results Encoder}{18}{subsection.3.2.3}\protected@file@percent }
\newlabel{subsec:results_enc}{{3.2.3}{18}{Results Encoder}{subsection.3.2.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.3}Shared Encoding}{18}{section.3.3}\protected@file@percent }
\newlabel{sec:shared_enc}{{3.3}{18}{Shared Encoding}{section.3.3}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {4}Results}{20}{chapter.4}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{chap:results}{{4}{20}{Results}{chapter.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.1}Weight Encoder}{20}{section.4.1}\protected@file@percent }
\newlabel{sec:weights}{{4.1}{20}{Weight Encoder}{section.4.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.2}Shared Encoder}{20}{section.4.2}\protected@file@percent }
\newlabel{sec:shared}{{4.2}{20}{Shared Encoder}{section.4.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.3}Conditional Model Sampling}{20}{section.4.3}\protected@file@percent }
\newlabel{sec:conditional_model_sample}{{4.3}{20}{Conditional Model Sampling}{section.4.3}{}}
\bibdata{mybib}
\@writefile{toc}{\contentsline {chapter}{\numberline {5}Summary}{21}{chapter.5}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{chap:conclusion}{{5}{21}{Summary}{chapter.5}{}}
\bibcite{Dean2017BlackBox}{1}
\bibcite{huggingface2024review}{2}
\bibcite{schurholt2022modelzoosdatasetdiverse}{3}
\bibcite{unterthiner2021predictingneuralnetworkaccuracy}{4}
\bibcite{salama2024datasetsizerecoverylora}{5}
\bibcite{schurholt2022hyperrepresentationsgenerativemodelssampling}{6}
\bibcite{pmlr-v235-schurholt24a}{7}
\bibcite{bedionita2025instructionguidedautoregressiveneuralnetwork}{8}
\bibcite{meynent2025structureenoughleveragingbehavior}{9}
\bibcite{radford2021learningtransferablevisualmodels}{10}
\@writefile{toc}{\contentsline {chapter}{Bibliography}{22}{chapter*.10}\protected@file@percent }
\bibcite{agren2022ntxentlossupperbound}{11}
\bibcite{NEURIPS2022_b2c4b7d3}{12}
\gdef \@abspage@last{26}
