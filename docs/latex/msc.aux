\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\babel@aux[2]{}
\@nameuse{bbl@beforestart}
\catcode `"\active 
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\bibstyle{IEEEtran}
\babel@aux{english}{}
\pgfsyspdfmark {pgfid1}{4661699}{49769019}
\@writefile{toc}{\contentsline {chapter}{Abstract}{i}{chapter*.1}\protected@file@percent }
\citation{Dean2017BlackBox}
\citation{huggingface2024review}
\citation{schurholt2022modelzoosdatasetdiverse}
\citation{unterthiner2021predictingneuralnetworkaccuracy}
\citation{salama2024datasetsizerecoverylora}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Introduction}{1}{chapter.1}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{chap:introduction}{{1}{1}{Introduction}{chapter.1}{}}
\citation{schurholt2022hyperrepresentationsgenerativemodelssampling}
\citation{pmlr-v235-schurholt24a}
\citation{bedionita2025instructionguidedautoregressiveneuralnetwork}
\citation{meynent2025structureenoughleveragingbehavior}
\citation{radford2021learningtransferablevisualmodels}
\citation{agren2022ntxentlossupperbound}
\@writefile{lof}{\contentsline {figure}{\numberline {1.1}{\ignorespaces The full pipeline of embedding a dataset, model weights and results in a shared embedding space.}}{3}{figure.caption.3}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:pipeline}{{1.1}{3}{The full pipeline of embedding a dataset, model weights and results in a shared embedding space}{figure.caption.3}{}}
\citation{}
\citation{}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Background}{4}{chapter.2}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{chap:background}{{2}{4}{Background}{chapter.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Weight Space Learning}{4}{section.2.1}\protected@file@percent }
\citation{}
\citation{}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Principal Component Analysis (PCA)}{5}{section.2.2}\protected@file@percent }
\newlabel{sec:pca}{{2.2}{5}{Principal Component Analysis (PCA)}{section.2.2}{}}
\newlabel{eq:pca}{{2.1}{6}{Principal Component Analysis (PCA)}{equation.2.2.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.3}Autoencoders}{6}{section.2.3}\protected@file@percent }
\newlabel{sec:autoencoders}{{2.3}{6}{Autoencoders}{section.2.3}{}}
\newlabel{eq:encoder}{{2.2}{7}{Autoencoders}{equation.2.3.2}{}}
\newlabel{eq:decoder}{{2.3}{7}{Autoencoders}{equation.2.3.3}{}}
\newlabel{eq:composite}{{2.4}{7}{Autoencoders}{equation.2.3.4}{}}
\newlabel{eq:ae_mse_loss}{{2.5}{7}{Autoencoders}{equation.2.3.5}{}}
\citation{NEURIPS2022_b2c4b7d3}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.1}Autoencoder for Neural Embeddings}{8}{subsection.2.3.1}\protected@file@percent }
\newlabel{eq:multi_loss}{{2.6}{8}{Autoencoder for Neural Embeddings}{equation.2.3.6}{}}
\newlabel{eq:weight_recon_loss}{{2.7}{8}{Autoencoder for Neural Embeddings}{equation.2.3.7}{}}
\citation{pmlr-v235-schurholt24a}
\citation{pmlr-v235-schurholt24a}
\citation{}
\citation{}
\@writefile{toc}{\contentsline {section}{\numberline {2.4}Contrastive Learning}{9}{section.2.4}\protected@file@percent }
\newlabel{sec:contrastive}{{2.4}{9}{Contrastive Learning}{section.2.4}{}}
\citation{}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.1}NT-Xent Loss}{10}{subsection.2.4.1}\protected@file@percent }
\newlabel{sec:nxtne_loss}{{2.4.1}{10}{NT-Xent Loss}{subsection.2.4.1}{}}
\newlabel{eq:nt-xent_loss}{{2.8}{10}{NT-Xent Loss}{equation.2.4.8}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.2}CLIP}{10}{subsection.2.4.2}\protected@file@percent }
\citation{agren2022ntxentlossupperbound}
\@writefile{lof}{\contentsline {figure}{\numberline {3.1}{\ignorespaces A figure illustrating the process of embedding a dataset, model weigths and results into a shared embedding space }}{11}{figure.caption.4}\protected@file@percent }
\newlabel{fig:pipeline_2}{{3.1}{11}{A figure illustrating the process of embedding a dataset, model weigths and results into a shared embedding space}{figure.caption.4}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Methodology}{11}{chapter.3}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{chap:method}{{3}{11}{Methodology}{chapter.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Data Generation}{12}{section.3.1}\protected@file@percent }
\newlabel{sec:data_gen}{{3.1}{12}{Data Generation}{section.3.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.2}Weight Encoder}{12}{section.3.2}\protected@file@percent }
\newlabel{sec:weight_enc}{{3.2}{12}{Weight Encoder}{section.3.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.3}Dataset Encoder}{13}{section.3.3}\protected@file@percent }
\newlabel{sec:data_enc}{{3.3}{13}{Dataset Encoder}{section.3.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.4}Results Encoder}{13}{section.3.4}\protected@file@percent }
\newlabel{sec:results_enc}{{3.4}{13}{Results Encoder}{section.3.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.5}Shared Encoding}{13}{section.3.5}\protected@file@percent }
\newlabel{sec:shared_enc}{{3.5}{13}{Shared Encoding}{section.3.5}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {4}Results and Analysis}{14}{chapter.4}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{chap:results}{{4}{14}{Results and Analysis}{chapter.4}{}}
\bibdata{mybib}
\@writefile{toc}{\contentsline {chapter}{\numberline {5}Summary and Conclusion}{15}{chapter.5}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{chap:conclusion}{{5}{15}{Summary and Conclusion}{chapter.5}{}}
\bibcite{Dean2017BlackBox}{1}
\bibcite{huggingface2024review}{2}
\bibcite{schurholt2022modelzoosdatasetdiverse}{3}
\bibcite{unterthiner2021predictingneuralnetworkaccuracy}{4}
\bibcite{salama2024datasetsizerecoverylora}{5}
\bibcite{schurholt2022hyperrepresentationsgenerativemodelssampling}{6}
\bibcite{pmlr-v235-schurholt24a}{7}
\bibcite{bedionita2025instructionguidedautoregressiveneuralnetwork}{8}
\bibcite{meynent2025structureenoughleveragingbehavior}{9}
\bibcite{radford2021learningtransferablevisualmodels}{10}
\@writefile{toc}{\contentsline {chapter}{Bibliography}{16}{chapter*.5}\protected@file@percent }
\bibcite{agren2022ntxentlossupperbound}{11}
\bibcite{NEURIPS2022_b2c4b7d3}{12}
\gdef \@abspage@last{20}
