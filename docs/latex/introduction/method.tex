\graphicspath{{fig/}}

\chapter{Methodology}
\label{chap:method}

Our hypothesis is that a unified representation space will enable conditional model sampling, allowing us to accurately approximate the conditional probability $P(W \mid \mathcal{D}, R)$. That is, we aim to sample model weights ($W$) conditioned on both specific dataset properties ($\mathcal{D}$) and target performance results ($R$).
    

To achieve this generative capability, our methodology requires a system that can project the three distinct data types—weights, datasets, and results—into a common, meaningful latent space. This mandates three core encoding components followed by a decoding mechanism for generation, the summation of these componets is visualised in Figure ~\ref{fig:pipeline}:


\begin{enumerate}
    \item \textbf{Weight Encoder and Decoder (${W} \rightarrow Z_W \rightarrow \hat{{W}}$):} To compress the high-dimensional weight tensor into a low-dimensional latent vector $Z_W$ and reconstruct the functional weights.
    \item \textbf{Dataset Encoder ($\mathcal{D} \rightarrow Z_{\mathcal{D}}$):} To map the dataset characteristics into a latent vector $Z_D$.
    \item \textbf{Results Encoder ($R \rightarrow Z_R$):} To map scalar performance metrics into a latent vector $Z_R$.
    \item \textbf{Shared Embedding/Alignment:} A mechanism to ensure the resultant latent vectors ($Z_W, Z_{\mathcal{D}}, Z_R$) are meaningfully aligned in a single, unified space $Z$.
\end{enumerate}

In Section~\ref{sec:data_gen}, we introduce a data generation approach that produces a diverse collection of $(\mathcal{D}, \mathcal{W}, \mathcal{R})$ triplets by systematically varying datasets, and optimisation parameters.

Section~\ref{sec:model} presents the design of the full embedding model, including the weight encoder and decoder,dataset encoder , and results embdding. The weight encoder model transforms the high-dimensional weight tensors ($\mathcal{W}$) into compact latent representations.  The dataset encoder maps each dataset ($\mathcal{D}$) into a semantic representation using pretrained CLIP features, while the results encoder transforms validation losses ($\mathcal{R}$) into fixed-size embeddings through a discretised lookup scheme. 

Finally, Section~\ref{sec:shared_enc} integrates these components within a shared latent space.The model learns to align the dataset and result embeddings with their corresponding weight representations, forming a unified space that captures the relationships between data, model parameters, and performance.

\section{Model Zoo Generation}
\label{sec:data_gen}
The construction of the model zoo is fundamentally driven by the need to generate a diverse, reproducible, and analytically valuable dataset of model weights for comprehensive weight-space exploration. 

The model zoo is composed of numerous models trained on tasks sampled from the same underlying distribution. For our study, this distribution is defined by 3-class classification problems drawn from ImageNet \cite{ImageNet_VSS09}, a widely used benchmark known for its diversity and richness of visual features. Restricting tasks to three randomly selected classes strikes a balance between computational tractability and the creation of a large number of distinct training instances. This design ensures that the models learn from high-quality, real-world visual data while providing a flexible and varied task space through the many possible class combinations.

The architecture was consistently set to a ResNet-18 backbone \cite{He2015DeepRL} across the entire zoo population, with only the final fully connected layer adapted for the three-class classification task. This architectural standardisation ensures that any observed differences in weight-space properties are primarily attributable to the systematic variations in training parameters and the task distribution. 

While early weight-space research often focused on smaller models, ResNet-18 introduces a more challenging architecture that remains compatible with modern approaches--enabling direct comparison to recent scalable methods such as \cite{pmlr-v235-schurholt24a}, while exceeding the limitations of earlier techniques like \cite{schurholt2022hyperrepresentationsgenerativemodelssampling}.

To guarantee the model zoo accurately reflects the diversity of the high-dimensional weight space, variation is introduced at the start of each training run. This begins with the random selection of classification classes from ImageNet, with the primary mechanism for systematic variation being the hyperparameter sampling from the distributions detailed in Table 1. Furthermore, the optimizer is randomly selected to be either Adam or Stochastic Gradient Descent (SGD), adding structural diversity by allowing distinct optimization paths.

% \begin{table}[!h]
%     \centering
%     \caption{Model Zoo Hyperparameter Sampling Distributions and Fixed Parameters}
%     \begin{tabularx}{0.8\linewidth}{@{}lX@{}}
%         \toprule
%         Hyperparameter & Distribution / Value \\
%         \midrule
%         Problem Domain & Random subsets of 3 ImageNet classes \\
%         Model Architecture & ResNet-18 (Fixed) \\
%         Optimizer & Randomly selected: Adam or SGD \\
%         Early Epoch Snapshot & Uniform, $U(5, 10)$ \\
%         Maximum Epoch & Truncated Normal, $N(\mu=100, \sigma=15)$ with bounds $[50, 150]$ \\
%         Learning Rate ($\eta$) & $Insert specific LR distribution$ \\
%         \bottomrule
%     \end{tabularx}
%     \label{tbl:model_zoo_params}
% \end{table}


\begin{table}[!h]
    \centering
    \caption{Model Zoo Hyperparameter Sampling Distributions and Fixed Parameters}
    \begin{tabularx}{0.95\linewidth}{@{}lXl@{}}
        \toprule
        Hyperparameter & Distribution / Value & Type \\
        \midrule
        Model Architecture & ResNet-18 & Fixed \\
        Problem Domain & Random subsets of 3 ImageNet classes & Sampled \\
        Optimizer & Randomly selected: Adam or SGD & Sampled \\
        Early Epoch Snapshot & Uniform, $U(5, 10)$ & Sampled \\
        Maximum Epoch & Truncated Normal, $N(100,15)$, clipped to $[50,150]$ & Sampled \\
        \bottomrule
    \end{tabularx}
    \label{tbl:model_zoo_params}
\end{table}


To analyze the geometry of the weight space at different phases of optimization, two distinct weights snapshots are captured per training trajectory: an 'early epoch' checkpoint and a 'max epoch' checkpoint. The early epoch is sampled uniformly, specifically designed to capture models in low-performing, under-trained states. This intentional approach provides crucial data on the structure of the weight space's nascent landscape and the optimization trajectory. Conversely, the maximum training epoch is sampled from a truncated Normal distribution. This range ensures the model has reached a point reflective of a converged, stable local loss minimum, thereby accurately representing the typical, high-performing outcomes of model training. Finally, the total sample size, $1000$, is set to the maximum number computationally feasible given resource constraints. This practical constraint ensures the generation of a statistically representative number of unique training instances necessary for robust analysis of the weight space.

\section{Model}
\label{sec:model}
\subsection{Weight Encoder}
\label{subsec:weight_enc}
The Weight Encoder is implemented to learn a compact, functional latent representation for neural network parameters. The objective is to train an autoencoder that encodes the parameters of each model and reconstructs them with minimal deviation from the originals. This corresponds to minimising the reconstruction loss, $\mathcal{L}_{\text{MSE}}$, as defined in Equation~\ref{eq:weight_recon_loss}. To ensure generalisation, the model is training using on the training set $\mathcal{M}_{\text{train}}$, and performance is validated on a separate validation set $\mathcal{M}_{\text{val}}$ with $100$, and $50$ samples held out for testing final performance metrics.


An intuitive baseline is to flatten the entire ResNet18 weight tensor $\mathbf{W}_m$ into a single high-dimensional vector and train an autoencoder directly on it. However, ResNet18 contains over 11 million parameters, so even a single linear layer mapping this input to a modest hidden dimension quickly becomes impractical. For instance, mapping the flattened weights ($\sim 11.7$M parameters) to a hidden dimension of 512 requires approximately 5.99 billion parameters. At 16-bit precision, this alone consumes nearly 12~GB of memory, demonstrating that a naive fully-connected approach is infeasible.

Alternative methods such as sequential tokenisation of layers (discussed in Section~\ref{sec:autoencoders}) provide a more scalable representation but introduce additional architectural complexity. Instead, we adopt a simpler yet effective two-stage compression approach, beginning with a \textit{Per-Named-Parameter PCA} transformation.  

In the first stage, dimensionality reduction is applied separately to each named parameter tensor (for example, \texttt{conv1.weight} or \texttt{fc.bias}) across all models. This preserves the modular structure of the network while tailoring the compression to the statistical properties of each parameter type. The process for determining which compression mode to apply is outlined in Algorithm~\ref{alg:pca-mode-selection}.

\begin{algorithm}[H]
\caption{Mode Selection During PCA Fitting}
\label{alg:pca-mode-selection}
\begin{algorithmic}[1]
\For{each named parameter in model zoo}
    \State $V \gets \text{calculate\_variance}(\text{parameter values across all models})$
    \State $D \gets \text{calculate\_dimension}(\text{parameter values across all models})$
    \If{$V < \text{VARIANCE\_THRESHOLD}$}
        \State \text{store\_only\_mean()} \Comment{Retain only mean}
    \ElsIf{$D \leq \text{DIMENSION\_LIMIT}$}
        \State \text{store\_centered\_weights()} \Comment{Retain centered weights}
    \Else
        \State \text{fit\_PCA()} \Comment{Fit PCA}
    \EndIf
\EndFor
\end{algorithmic}
\end{algorithm}

Three compression modes are used:
\begin{itemize}
    \item For high-variance, high-dimensional tensors, \textit{IncrementalPCA} projects the centred weights onto a small number of principal components, retaining maximal variance in a compact coefficient representation.
    \item For small tensors (four dimensions or fewer), only centering is applied, as PCA provides negligible benefit.
    \item For low-variance tensors, only the mean value is stored, discarding coefficients entirely for maximal compression.
\end{itemize}

Following PCA, the resulting coefficient vectors obtained for each named parameter are concatenated to form a single reduced representation for each model. Each vector contains the retained PCA coefficients for one named parameter, where the number of retained components \(k_i\) varies depending on that parameter's variance structure. The concatenated coefficient vector, denoted \(\mathbf{C}_m\), thus compactly represents all named parameters of model \(m\) in a unified format. This vector is normalised using dataset-wide statistics and then serves as input to the second compression step, a deep autoencoder. This two-stage compression process is illustrated in Figure~\ref{fig:pca_ae}, showing how PCA is applied per named parameter followed by a deep autoencoder producing a compact latent representation.


\begin{figure}[!t]
    \centering
    \includegraphics[width=0.9\linewidth]{pca_ae.png}
    \caption{Two-stage compression of model weights: PCA reduces each parameter tensor, followed by an autoencoder producing a compact latent representation.}

    \label{fig:pca_ae}
\end{figure}

% You never describe what theta_e or theta_d is
In the second stage, the autoencoder learns a lower-dimensional latent representation of the PCA-compressed vectors. The encoder maps the normalised coefficient vector $\mathbf{C}_m$ to a compact latent representation $\mathbf{z}_m \in \mathbb{R}^{D_{\text{latent}}}$:
\[
\mathbf{z}_m = f_{\text{enc}}(\mathbf{C}_m; \theta_e),
\]
where $\theta_e$ denotes the learnable parameters of the encoder. The decoder reconstructs the original coefficients as
\[
\hat{\mathbf{C}}_m = f_{\text{dec}}(\mathbf{z}_m; \theta_d).
\]
with $\theta_d$ representing the learnable parameters of the decoder. 
% Won't go that far, will more indicate that it minisises the **reconstruction loss** of the PCA vector.


Training minimises the reconstruction loss of the PCA-compressed model weights, extending the standard mean-squared error loss in Equation~\ref{eq:weight_recon_loss}:

\begin{equation}
  \mathcal{L}_{\text{AE}} = \frac{1}{|\mathcal{M}_{\text{train}}|} 
\sum_{m \in \mathcal{M}_{\text{train}}} 
\| \mathbf{C}_m - f_{\text{dec}}(f_{\text{enc}}(\mathbf{C}_m)) \|^2_2,
  \label{eq:pca_ae_loss_simplified}
\end{equation}

where $\mathbf{C}_m$ denotes the PCA coefficient vector representing all weights of model $m$. This formulation encourages the autoencoder to capture the overall structure of each model in a compact latent space while preserving the key variance across the full set of parameters.


After reconstruction, each parameter’s coefficients $\hat{\mathbf{c}}_{i,m}$ are transformed back into their original tensor shapes using the stored PCA statistics from the fitting stage. Specifically, for each named parameter $i$, the inverse PCA transform restores the weight tensor by reprojecting the coefficients into the original feature space and re-adding the mean vector:
\[
\hat{\mathbf{W}}_{i,m} = \mathbf{V}_i \hat{\mathbf{c}}_{i,m} + \boldsymbol{\mu}_i,
\]
where $\mathbf{V}_i$ is the PCA component matrix and $\boldsymbol{\mu}_i$ is the mean of the original parameter distribution. If PCA was not applied (for small or low-variance tensors), the stored centring or mean operations are inverted accordingly. 

The combination of PCA and an autoencoder provides several advantages. Applying PCA first drastically reduces the dimensionality of each model's weight vector, lowering both computational and memory demands for training the autoencoder. This compression enables faster and more efficient training while still capturing the dominant variance patterns present in the model weights.

Despite these benefits, the approach has inherent limitations. PCA is a linear technique, which means it can only approximate the weight space along linear directions and may miss non-linear dependencies between parameters. Additionally, it compresses based on the directions of variance observed in the training models, so models with weight configurations that vary along previously unrepresented directions may also be poorly encoded. Both factors can limit generalisation, particularly when the autoencoder encounters models with parameter distributions that differ substantially from the training set.

\subsection{Dataset Encoder}
\label{subsec:data_enc}

Following the discussion in Section~\ref{sec:contrastive} on contrastive learning and the CLIP framework, we adopt a pretrained CLIP visual encoder to extract semantic embeddings for the dataset classes. Each image $x$ is passed through the CLIP encoder $f_{\text{CLIP}}(\cdot)$, producing a 512-dimensional latent representation $z_x$:

\[
z_x = f_{\text{CLIP}}(x).
\]

To obtain a class-level representation, we compute the embeddings for all images belonging to a given class and average them, yielding a single 512-dimensional vector $\bar{z}_c$ that summarises the class:

\[
\bar{z}_c = \frac{1}{|X_c|} \sum_{x \in X_c} z_x,
\]

where $X_c$ denotes the set of images in class $c$. 

Each \(\bar{z}_{c_i}\) represents the embedding of a specific class in the dataset, with \(c_1, c_2, c_3\) consistently ordered according to the class indices in the classification layer. This consistent ordering preserves structure in the concatenated vector 

\[
\mathbf{z}_{\text{dataset}} = [\bar{z}_{c_1}; \bar{z}_{c_2}; \bar{z}_{c_3}] \in \mathbb{R}^{1536},
\]

which is then used as input to the shared encoding stage for alignment with the weight embeddings. By leveraging pretrained CLIP features, this approach provides a rich semantic summarisation of each class, ensuring the final dataset representation captures meaningful visual information relevant for model alignment.


\subsection{Results Encoder}
\label{subsec:results_enc}

To capture the performance of each model, we use the validation loss recorded at the checkpoint corresponding to the saved weights. Instead of using the raw scalar loss directly, we quantize the range of validation losses observed across the model zoo into discrete bins. Each bin is associated with a learnable 512-dimensional embedding vector, allowing the model to map a validation loss to a rich latent representation:

\[
\mathbf{r}_m = \text{Embedding}(\text{bin}(\mathcal{L}_{\text{val}, m})),
\]

where $\mathcal{L}_{\text{val}, m}$ is the validation loss of model $m$, and $\mathbf{r}_m \in \mathbb{R}^{512}$ is the corresponding learned embedding.  

This approach has several advantages. First, by representing the continuous range of validation losses with trainable embeddings, the system can learn a smooth latent space that captures nuanced performance differences between models. Second, it avoids the need to directly regress continuous loss values, which can be noisy and poorly scaled across diverse architectures or training schedules.

\section{Shared Encoding}
\label{sec:shared_enc}

The shared encoding stage aligns the dataset and results embeddings with the weight latent space. To achieve this, the dataset-level vector $\mathbf{z}_{\text{dataset}}$ from Section~\ref{subsec:data_enc} and the results embedding $\mathbf{z}_{\text{results}}$ from Section~\ref{subsec:results_enc} are first concatenated:

\[
\mathbf{z}_{\text{input}} = [\mathbf{z}_{\text{dataset}}; \mathbf{z}_{\text{results}}].
\]

This combined vector is then passed through a multi-layer perceptron (MLP) to produce a predicted embedding $\mathbf{z}_{\text{proj}}$ in the weight latent space:

\[
\mathbf{z}_{\text{proj}} = \text{MLP}(\mathbf{z}_{\text{input}}; \theta_{\text{MLP}}).
\]
s
Training minimises the NT-Xent contrastive loss, as described in Section~\ref{sec:contrastive} and defined in Equation~\ref{eq:multi_loss}, between $\hat{\mathbf{z}}_{\text{weight}}$ and the corresponding weight embedding $\mathbf{z}_{\text{weight}}$. Backpropagation is used to update the MLP parameters $\theta_{\text{MLP}}$:

\[
\mathcal{L}_{\text{NT-Xent}}(\mathbf{z}_{\text{proj}}, \mathbf{z}_{\text{weight}}) \rightarrow \min_{\theta_{\text{MLP}}}.
\]

This design ensures that the mapping from dataset and results space to the weight latent space is fully differentiable and preserves the reversibility of the weight representation. Importantly, the projection is performed from dataset and results to weight space rather than the reverse: applying a non-linear transformation directly to the weight embedding would break invertibility, making it impossible to reconstruct the original weight vector from the shared representation. By contrast, pushing the dataset and results embeddings to the weight space allows the latent weight vector to remain consistent and decodable via the trained autoencoder.

\begin{figure}[!t]
  \centering
  \includegraphics[width=0.918\linewidth]{conditional_model_sampling.png}
  \caption{Conditional model sampling pipeline shown, from dataset and results embedding, to a shared embedding space, to a reconstructed model}
  \label{fig:conditional}
\end{figure}

The overall conditional model sampling process is illustrated in Figure~\ref{fig:conditional}. Given a dataset embedding \(\mathbf{z}_{\text{dataset}}\) and results embedding \(\mathbf{z}_{\text{results}}\), the shared encoder projects their concatenated representation into the latent weight space as \(\mathbf{z}_{\text{proj}}\). Once aligned, this predicted latent vector can be passed through the trained weight decoder to reconstruct the full set of model parameters. In effect, this enables conditional generation of neural networks—sampling model weights that are consistent with a given dataset and desired performance profile.
