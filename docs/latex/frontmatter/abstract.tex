\chapter*{Abstract}
\addcontentsline{toc}{chapter}{Abstract}
\makeatletter\@mkboth{}{Abstract}\makeatother
% TODO {abstract} 
Neural networks have become a cornerstone of modern artificial intelligence, but challenges remain in understanding their internal representations and efficiently generating new models. Weight space learning offers a framework for addressing these challenges by modelling the structure of neural network parameters and their relationship to datasets and performance. In this work, we propose a contrastive learning approach that jointly embeds neural network weights, the datasets they are trained on, and their resulting performance metrics into a unified latent space. Separate encoders for dataset and weight representations, combined with a binned results embedding, are trained with a contrastive objective to align these heterogeneous modalities. This unified embedding space enables interpretability—by revealing relationships between datasets and model behaviour—and conditional model sampling, approximating the joint distribution \(P(W \mid \mathcal{D}, R)\). Experimental results highlight limitations in current weight encoding strategies, particularly linear PCA compression, which constrain generalisation. Nonetheless, the framework demonstrates the potential of contrastively aligning heterogeneous modalities, providing a foundation for future work aimed at improving embedding quality, expanding model diversity, and enhancing conditional model generation.

% TODO {abstract} 

