\chapter{Results}
\label{chap:results}
% Report the measurement -> what does this mean? 
% Direct in how it is stated.




\section{Weight Autoencoder}
\label{sec:weights}

\begin{figure*}[!t]
    \centering
    \subfloat[Training and Validation Loss]{
    \includegraphics[width=0.48\textwidth]{weight_encoder/combined_loss_plot.png}
        \label{fig:weight_encoder_performance}
    }
    \subfloat[Test Model Output Comparison]{
    \includegraphics[width=0.48\textwidth]{weight_encoder/output_comparison.png}
        \label{fig:output_comparison}
    }
    \caption{Performance curves of the Weight Autoencoder, showing loss progression and comparative output metrics.}
    \label{fig:combined_plots}
\end{figure*}

PCA is initially applied to the training set of the Model Zoo, after which the autoencoder is subsequently trained.

To evaluate the efficacy of the Weight Autoencoder, the loss function evolution during training is analyzed. Figure \ref{fig:weight_encoder_performance} illustrates the loss function for the Weight Autoencoder following the PCA projection. The training loss consistently decreases over epochs, eventually reaching a plateau. The validation loss exhibits a similar trajectory, decreasing as training progresses. This consistent performance indicates that the model is not overfitting to the training data and demonstrates a capacity for generalization to unseen data. Specifically, this suggests that the Weight Autoencoder has successfully learned to encode and decode model representations beyond the training set, enabling the generation and reconstruction of effective latent representations for the designated task.

The MSE Loss, depicted in Figure \ref{fig:weight_encoder_performance}, confirms that the Weight Autoencoder is progressing in solving the reconstruction task. However, since the magnitude of the error is relative to the scale of the input model weights, it is not directly interpretable as a measure of preserved model performance; thus, a comparative evaluation is requisite.

Figure \ref{fig:output_comparison} presents the results from a comparative experiment: unseen models are encoded and reconstructed, and the reconstructed models are then utilized to classify a specific test dataset. The output of the original model is compared against the output of the reconstructed model. Throughout the training, the model agreement on synthetic images was $100\%$. Model agreement is defined as the percentage of test samples for which the classified label is identical between the original and reconstructed models. Correlation and cosine similarity were calculated on the pre-softmax outputs. It is evident that the Weight Autoencoder is capable of learning model representations with sufficient fidelity to reconstruct models exhibiting highly correlated outputs---specifically, $0.95+\%$ correlation---compared to the original model's outputs.

The same model output comparison procedure is then executed, but using the data upon which the original model was initially trained. Table \ref{tab:weight_real_data} summarizes these cross-dataset performance metrics.

\begin{table}[!h]
    \centering
    \caption{Model Reconstruction Performance on Training Data}
    \label{tab:weight_real_data}
    \begin{tabularx}{0.8\linewidth}{@{}lX@{}} 
        \toprule
        Average Metric & Value \\
        \midrule
        Cosine Similarity (Pre-Softmax Output) & $-0.0323$ \\
        Correlation (Pre-Softmax Output) & $-0.08570$ \\
        Predition Agreement & $35.6905 \% $ \\
        \bottomrule
    \end{tabularx}
\end{table}

It is clear from these metrics that the Weight Autoencoder fails to reconstruct models with enough fidelity to preserve the original decision boundaries. The large drop in output correlation between the reconstructed and real models suggests that, while a coarse representation of the ResNet18 weights is captured, critical fine-grained information required for functional accuracy is lost.  

This degradation is likely caused by the initial PCA compression stage. Although PCA theoretically retains the principal directions of variance and thus the most informative components, it remains a linear approximation of a highly non-linear parameter space. Consequently, important non-linear relationships within the weights may have been discarded during projection. Given that the Autoencoder itself achieves low reconstruction loss and generalises well, as shown in Figure~\ref{fig:weight_encoder_performance}, the poor reconstruction performance can reasonably be attributed to information loss introduced by PCA.

\(\textbf{NOTE:}\)
When I tried to use a more expressive model the validaiotn loss of the autoencoder kept increasing for decreasing training loss, only the smallest possible model seemed to do well, no hidden layers. This indicates to me severe overfitting, despite the model being relatively small. This may be yet another indicaiton of the fact that the PCA compression did not retain sufficient information, as a more expressive encoder would have led to better results. Should I include a plot of this with my logic and conclusions outlined as above ?

\(\textbf{NOTE:}\)
Also when I used a per param pca componets value of 64 and a more expresive model, reconstrcution and output performance metrics improved, though marginally.
the NXT-loss went up, but this is expected and the reconstruction is what matters. This may indicate that shifting to more compression in the encoder and less in the PCA may lead to better performance. should I include these plots and discussion as well?


\section{Shared Encoder}
\label{sec:shared}

The Shared Encoder was trained following the procedure described in Section~\ref{sec:shared_enc}, with both training and validation losses monitored throughout. Figure~\ref{fig:shared_loss} shows that the training loss decreases rapidly at first and then decreases at a steady rate, while the validation loss decreases slightly before stabilising at a relatively high value of \(\sim\)3.1.

This behaviour suggests that the Shared Encoder is successfully fitting the training data but fails to generalise to unseen data. The persistently high validation loss indicates that the model cannot extract a meaningful or consistent signal from the validation set, likely due to information loss introduced earlier in the weight encoding process. As the weight representations lack sufficient structure or variance relevant to the corresponding $(\mathcal{D}, R)$ pairs, the Shared Encoder effectively trains on noise. 

Interestingly, the validation loss does not worsen over time, which would typically occur during overfitting. This further supports the hypothesis that there is little to no informative signal to learnâ€”changes to the Shared Encoder's weights have minimal effect on generalisation performance, implying that the training dynamics are dominated by random correlations rather than meaningful alignment.

\begin{figure}[!t]
    \centering
    \includegraphics[width=0.75\linewidth]{shared/nt_xent_loss_plot.png}
    \caption[Shared Encoder training and validation losses.]{Training and validation losses for the Shared Encoder. The validation loss remains high, indicating a lack of generalisable signal due to information loss in the weight embeddings.}
    \label{fig:shared_loss}
\end{figure}

Figure~\ref{fig:heat_map} provides further insight into this behaviour by visualising the cosine similarity between five weight embeddings and their corresponding projected embeddings ${z}_{\text{proj}}$ within the training set. The x-axis represents $(\mathcal{D}, R)$ pairs at both early and late training epochs, while the y-axis shows the corresponding weight latent vectors from the Weight Encoder.

High cosine similarity values (red regions) are observed along the diagonal, showing that matching pairs of weights and $(\mathcal{D}, R)$ embeddings are aligned in the shared space. Negative pairs, shown as white or blue, exhibit low similarity as expected. Notably, the same $(\mathcal{D}, R)$ pairs across different epochs also display higher similarity, despite not being explicitly defined as positives. This suggests that the Shared Encoder partially retains the relational structure among triplets $(\mathcal{D}, R, W)$ across training stages, even in the absence of a strong global signal.

\begin{figure}[!t]
    \centering
    \makebox[\textwidth][c]{\hspace{-1cm}\includegraphics[width=1.2\linewidth]{heat_map.png}}
    \caption[Cosine similarity heat map for weight and projected embeddings.]{Cosine similarity heat map between five weight embeddings and their corresponding projected embeddings $z_{\text{proj}}$.}
    \label{fig:heat_map}
\end{figure}

