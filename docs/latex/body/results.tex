\chapter{Results}
\label{chap:results}
% Report the measurement -> what does this mean? 
% Direct in how it is stated.




\section{Weight Autoencoder}
\label{sec:weights}

\begin{figure*}[!t]
    \centering
    \subfloat[Training and Validation Loss]{
    \includegraphics[width=0.48\textwidth]{weight_encoder/combined_loss_plot.png}
        \label{fig:weight_encoder_performance}
    }
    \subfloat[Test Model Output Comparison]{
    \includegraphics[width=0.48\textwidth]{weight_encoder/output_comparison.png}
        \label{fig:output_comparison}
    }
    \caption{Performance curves of the Weight Autoencoder, showing loss progression and comparative output metrics.}
    \label{fig:combined_plots}
\end{figure*}

PCA is initially applied to the training set of the Model Zoo, after which the autoencoder is subsequently trained.

To evaluate the efficacy of the Weight Autoencoder, the loss function evolution during training is analyzed. Figure \ref{fig:weight_encoder_performance} illustrates the loss function for the Weight Autoencoder following the PCA projection. The training loss consistently decreases over epochs, eventually reaching a plateau. The validation loss exhibits a similar trajectory, decreasing as training progresses. This consistent performance indicates that the model is not overfitting to the training data and demonstrates a capacity for generalization to unseen data. Specifically, this suggests that the Weight Autoencoder has successfully learned to encode and decode model representations beyond the training set, enabling the generation and reconstruction of effective latent representations for the designated task.

The MSE Loss, depicted in Figure \ref{fig:weight_encoder_performance}, confirms that the Weight Autoencoder is progressing in solving the reconstruction task. However, since the magnitude of the error is relative to the scale of the input model weights, it is not directly interpretable as a measure of preserved model performance; thus, a comparative evaluation is requisite.

Figure \ref{fig:output_comparison} presents the results from a comparative experiment: unseen models are encoded and reconstructed, and the reconstructed models are then utilized to classify a specific test dataset. The output of the original model is compared against the output of the reconstructed model. Throughout the training, the model agreement on synthetic images was $100\%$. Model agreement is defined as the percentage of test samples for which the classified label is identical between the original and reconstructed models. Correlation and cosine similarity were calculated on the pre-softmax outputs. It is evident that the Weight Autoencoder is capable of learning model representations with sufficient fidelity to reconstruct models exhibiting highly correlated outputs---specifically, $0.95+\%$ correlation---compared to the original model's outputs.

The same model output comparison procedure is then executed, but using the data upon which the original model was initially trained. Table \ref{tab:weight_real_data} summarizes these cross-dataset performance metrics.

\begin{table}[!h]
    \centering
    \caption{Model Reconstruction Performance on Training Data}
    \label{tab:weight_real_data}
    \begin{tabularx}{0.8\linewidth}{@{}lX@{}} 
        \toprule
        Average Metric & Value \\
        \midrule
        Cosine Similarity (Pre-Softmax Output) & $-0.0323$ \\
        Correlation (Pre-Softmax Output) & $-0.08570$ \\
        Predition Agreement & $35.6905 \% $ \\
        \bottomrule
    \end{tabularx}
\end{table}



It is clear from these metrics that the Weight Autoencoder is unable to reconstruct models with sufficient quality to preserve the decision boundary of the original model. The significant discrepancy in output correlation between the synthetic and real data indicates that while a representation of the ResNet18 model is being learned, it lacks the necessary fine-grained resolution required for accurate reconstruction of functional models.

\section{Shared Encoder}
\label{sec:shared}
% Overall Loss
%     Val
%     Train 

% Distance matrix
%     heatmap/ example embedings showing well seperatedness
The NT-Xent loss is minized on a traing set of model weights, dataset and results, with a 

\begin{figure}[!t]
    \centering
    \includegraphics[width=0.75\linewidth]{shared/nt_xent_loss_plot.png}
    \caption[]{}
    \label{fig:shared_loss}
\end{figure}




\subsection{Conditional Model Sampling}
\label{subsec:conditional_model_sample}

% money Shot 
%  input accuracy vs output accuracy