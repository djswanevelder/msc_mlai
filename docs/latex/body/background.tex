\chapter{Background}
\label{chap:background}
\section{Weight Space Learning}

Weight space learning is a field within machine learning that focuses on understanding and leveraging the structure of the neural network weight space. Its central aim is to model how network parameters are shaped by data, architecture, and training dynamics, and to capture these relationships within a learnable representation. 

The weight space \(W\) of a neural network is formally defined as the high-dimensional Euclidean space \(\mathbb{R}^N\) spanned by the \(N\) learnable parameters \cite{pmlr-v38-choromanska15}—the weights and biases—of the specified architecture. A specific point \(\mathbf{w} \in W\) represents a complete instantiation of the model, mapping input data \(\mathbf{x}\) to output predictions \(f(\mathbf{x}, \mathbf{w})\).  

Weight space learning can be broadly divided into generative and discriminative tasks. Generative approaches aim to characterise the distribution of neural network weights, potentially conditioned on auxiliary information or reference models, \(P(W \mid \dots)\), enabling the synthesis of novel weight configurations.  

For instance, \cite{schurholt2022hyperrepresentationsgenerativemodelssampling} employed an autoencoder to create compact representations of model zoos, with the decoder being used to generate new weights. Building on this concept, \cite{pmlr-v235-schurholt24a} proposed the Sequential Autoencoder for Neural Embeddings (SANE), which introduces weight tokenisation and sequential embedding to improve scalability for larger networks.  

Other methods focus on conditioning the weight distribution on dataset or performance information. \cite{bedionita2025instructionguidedautoregressiveneuralnetwork} employed a Vector Quantised Variational Autoencoder (VQ-VAE) to model \(P(W \mid \mathcal{D})\), learning latent representations that allow generation of new weights for specific tasks. Similarly, \cite{meynent2025structureenoughleveragingbehavior} modelled \(P(W \mid R)\) using a contrastive objective that minimises differences in both model outputs and weights between original and reconstructed networks, effectively embedding behavioural information into the embedding space.  

Discriminative weight space learning, in contrast, uses pre-trained network weights—often aggregated in model zoos \cite{schurholt2022modelzoosdatasetdiverse}—to infer meta-information about the original models \cite{unterthiner2021predictingneuralnetworkaccuracy}. Representations are typically assessed by training a simple predictor, such as an MLP, to map embeddings to model metrics, including final performance or generalisation gap. These tasks demonstrate that weight embeddings encode meaningful training information, providing both a benchmark for representation quality and practical predictive utility.

Discriminative and generative tasks alike operate on high-dimensional weight spaces, which can consist of millions of parameters in modern networks. To manage this complexity, latent representations are used to encode the essential information of the weights into a more compact form. Such representations encode a model's parameters into a compact embedding space through learned transformations—either linear or non-linear—that preserve essential information about the original weights. A key distinction among dimensionality reduction methods is whether they are reversible or non-reversible.

Reversibility is particularly important in weight space learning. While encoding weights into a latent representation (real \(\rightarrow\) latent) is informative, the ability to reconstruct the original weights (real \(\rightarrow\) latent \(\rightarrow\) real) is far more valuable. This reversibility enables the synthesis of entirely new weight configurations, supporting generative applications such as zero-shot model creation and performance-guided model generation. Consequently, reversible latent representation methods are the most prevalent within weight space learning, especially for encoding and decoding neural network weights.

We next examine two fundamental approaches to creating such reversible representations: Principal Component Analysis (PCA), which provides linear but interpretable transformations, and autoencoders, which enable more expressive non-linear mappings. We begin with PCA as it establishes the baseline for reversible dimensionality reduction.


\section{Principal Component Analysis (PCA)}
\label{sec:pca}

Principal Component Analysis (PCA) is a linear dimensionality reduction technique used to represent high-dimensional data in a lower-dimensional form while retaining as much variance as possible. It provides a compact latent representation that captures the most informative directions of variation in the data.

Consider a dataset \( X \in \mathbb{R}^{n \times d} \) with \( n \) samples and \( d \) features, centred such that each feature has zero mean. The goal of PCA is to find a new coordinate system whose axes are linear transformations of the original dimensions, ordered by the amount of variance they capture. Orthogonality between these axes ensures that each captures unique, non-redundant information about the data.

Intuitively, PCA identifies the directions that best describe the “shape” or spread of the data cloud in feature space. Projecting the data onto the top \( k \) directions provides a compressed representation that preserves most of the information while discarding redundancy.

PCA seeks a linear projection matrix \( W \in \mathbb{R}^{d \times k} \) that maps the data to a lower-dimensional space:
\begin{equation}
    Z = X W
  \label{eq:pca}
\end{equation}
where \( Z \in \mathbb{R}^{n \times k} \) represents the latent representation.
Assuming \( X \) is already centered the sample covariance matrix simplifies to 
\[
\Sigma = \frac{1}{n-1} X^\top X,
\]
and the total variance captured by the projection is
\[
\text{Var}(Z) = \text{Tr}(W^\top \Sigma W),
\]
where the trace operator \( \text{Tr}(\cdot) \) sums the variances along all projected directions.  

PCA thus maximises the variance of the projected data:
\[
\max_{W} \text{Tr}(W^\top \Sigma W)
\quad \text{subject to} \quad W^\top W = I_k.
\]
The constraint enforces orthonormality among the new axes so that each captures distinct variance. Solving this optimisation leads to the eigenvalue problem:
\[
\Sigma W = W \Lambda,
\]
where the columns of \( W \) are the eigenvectors of \( \Sigma \), and the diagonal entries of \( \Lambda \) are the corresponding eigenvalues that quantify the variance explained by each principal component. The top \( k \) eigenvectors define the optimal projection directions.
The resulting embedding, \(Z\) is the \textit{latent representation} of the data, capturing the dominant linear structure of the dataset.  

The original data can be approximately reconstructed from the embedding space using the transpose of the transformation matrix:
\begin{equation}
    \hat{X} = Z W_k^\top
  \label{eq:pca_reconstruction}
\end{equation}
where $\hat{X} \in R^{n \times d}$ is the low-rank approximation of the centered data $X$.

PCA provides a linear method for generating lossy reversibility latent representations, however, it's linear nature limits its ability to capture non-linear relationships, thereby restricting its overall expressiveness.


\section{Autoencoders}
\label{sec:autoencoders}
Autoencoders are neural network architectures designed for unsupervised representation learning, as illustrated in Figure~\ref{fig:autoencoder}. Their primary goal is to learn a compressed, informative latent representation of input data by training the system to reconstruct the original input after it passes through a lower-dimensional bottleneck layer \cite{Hinton2006Reducing}. The full architecture consists of an encoder and a decoder. Crucially, the layers within these components typically employ non-linear activation functions, enabling the model to perform complex, non-linear mappings from the high-dimensional input space to the embedding space. This capability is essential for capturing intricate, hierarchical structures within the data that linear dimensionality reduction techniques cannot access.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.5\linewidth]{encoder.png}
    \caption{Autoencoder encoding input data into a latent representation and decoding it to reconstructed data}
    \label{fig:autoencoder}
\end{figure}

An autoencoder consists of two primary components: an \textit{encoder} and a \textit{decoder}. The encoder, parameterised by weights $\theta_e$, maps an input $x \in \mathbb{R}^d$ to a latent representation $z \in \mathbb{R}^k$ through a sequence of nonlinear transformations:
\begin{equation}
z = f_\text{enc}(x; \theta_e).
\label{eq:encoder}
\end{equation}
The decoder, parameterised by $\theta_d$, reconstructs the input from this latent representation:
\begin{equation}
\hat{x} = f_\text{dec}(z; \theta_d).
\label{eq:decoder}
\end{equation}
Together, the encoder and decoder form a composite function:
\begin{equation}
\hat{x} = f_\text{dec}(f_\text{enc}(x)).
\label{eq:composite}
\end{equation}
The bottleneck layer (embedding space) enforces a compression constraint, ensuring the model retains only the most salient features necessary for accurate reconstruction.


The network is trained to minimise the reconstruction error between the input $x$ and its reconstruction $\hat{x}$. The most common loss function is the Mean Squared Error (MSE):
\begin{equation}
\mathcal{L} = \frac{1}{n} \sum_{i=1}^{n} \| x_i - \hat{x}_i \|^2.
\label{eq:ae_mse_loss}
\end{equation}
By minimising this objective, the autoencoder learns weight configurations that best compress and reconstruct the data distribution.

The embedding space $z$ provides a nonlinear embedding that captures underlying structure within the data. Once trained, the encoder can be used independently for dimensionality reduction or feature extraction, while the decoder can serve as a generative mapping from the embedding space back to the input domain. 
\section{Contrastive Learning}
\label{sec:contrastive}
The overall goal of contrastive learning is to learn meaningful representations by comparing data points against one another, rather than relying on explicit labels \cite{foundationsCVbook}. The foundational assumption is that if a model is provided with sufficient examples of similar and dissimilar pairs, it can learn to represent data such that semantically similar samples lie close together in the embedding space, while dissimilar samples are placed farther apart. This framework enables unsupervised or self-supervised learning of latent representations that capture semantic structure purely through relative similarity.

\begin{figure}[!t]
    \centering
    \includegraphics[width=0.65\linewidth]{contrastive.png}
    \caption{Contrastive encoder pulling similar data pairs $(x,x^+)$ together in embedding space and pushing dissimilar pairs $(x,x^-)$ apart \cite{foundationsCVbook}.}
    \label{fig:contrastive}
\end{figure}

\subsection{NT-Xent Loss}
\label{sec:ntxne_loss}
A widely used objective for contrastive learning is the Normalised Temperature-Scaled Cross Entropy (NT-Xent) loss introduced in \cite{pmlr-v119-chen20j}. This loss formulates the contrastive task as a classification problem over positive and negative pairs within a batch.  
Given a batch of $N$ samples, each with an augmented pair $(x_i, x_i')$, the loss for a positive pair $(i, j)$ is defined as:
\begin{equation}
  \mathcal{L}_{i,j} = -\log \frac{\exp(\text{sim}(z_i, z_j) / \tau)}{\sum_{k=1}^{2N} \mathbb{1}_{[k \neq i]} \exp(\text{sim}(z_i, z_k) / \tau)}
  \label{eq:nt-xent_loss}
\end{equation}
where $\text{sim}(z_i, z_j)$ denotes the cosine similarity between the latent representations $z_i$ and $z_j$, and $\tau$ is a temperature parameter that controls the sharpness of the distribution. The total batch loss is obtained by averaging over all positive pairs.  
This formulation encourages positive pairs (augmentations of the same sample) to have high similarity, while simultaneously pushing apart embeddings from different samples, shown visually in Figure~\ref{fig:contrastive}, resulting in well-structured and discriminative latent representations.

\subsection{Multimodal Embedding}

A key advancement enabled by contrastive learning is the creation of multimodal embedding spaces, which align representations from distinct data types, such as images, text, and neural network weights, into a single, shared embedding space. The importance of this lies in its ability to facilitate cross-modal understanding and retrieval. By representing different data types with vectors in the same space, models can compute the semantic similarity between, for example, an image and a piece of text directly. 

An important application of contrastive learning using the NT-Xent objective is CLIP (Contrastive Language--Image Pretraining) \cite{pmlr-v139-radford21a}. CLIP jointly trains an image encoder and a text encoder to align visual and textual representations in a shared embedding space.  
During training, each image is paired with a corresponding caption, forming a positive pair, while all other image--text combinations in the batch act as negatives. The model optimises a symmetric contrastive objective, where each image predicts its matching caption and vice versa:
\[
\mathcal{L}_{\text{CLIP}} = \frac{1}{2}(\mathcal{L}_{\text{image-to-text}} + \mathcal{L}_{\text{text-to-image}}).
\]
The loss components, $\mathcal{L}_{\text{image-to-text}}$ and $\mathcal{L}_{\text{text-to-image}}$, are calculated using a contrastive objective. During training, a batch of $N$ image-caption pairs is sampled. The paired image and text constitute the single positive pair against which the model optimizes. All other $N-1$ image-text combinations within that batch are treated as negative pairs. The overall objective $\mathcal{L}_{\text{CLIP}}$ is symmetric:

\begin{itemize}
    \item $\mathcal{L}_{\text{image-to-text}}$ treats the images as anchor points, where the loss is calculated by having each image predict its correct matching caption from the $N$ available captions in the batch.
    \item $\mathcal{L}_{\text{text-to-image}}$ treats the captions as anchor points, where the loss is calculated by having each caption predict its correct matching image from the $N$ available images in the batch.
\end{itemize}

The total loss is the average of these two directional cross-entropy terms, ensuring that the embeddings are aligned symmetrically.
Through this process, CLIP learns general-purpose visual and linguistic representations that are semantically aligned. Once trained, the model can perform zero-shot classification and other cross-modal tasks by measuring similarity between image and text embeddings without task-specific fine-tuning.


\section{Autoencoder for Neural Embeddings}
Applying the concept of an autoencoder to neural network weights has recently gained attention as a means to learn structured, low-dimensional representations of model parameters. Instead of encoding raw input data, the autoencoder learns to encode and reconstruct the weights of pre-trained neural networks, effectively embedding each model into a embedding space that captures structural and functional similarities between models. This approach was explored in \cite{NEURIPS2022_b2c4b7d3}, where the goal was to build a continuous and interpretable representation space of neural weights.


Unlike conventional autoencoders that optimise a single reconstruction loss, the neural weight autoencoder is trained with a multi-objective loss function that combines reconstruction and contrastive terms:
\begin{equation}
\mathcal{L} = \beta \mathcal{L}_{\text{MSE}} + (1 - \beta) \mathcal{L}_{\text{c}},
\label{eq:multi_loss}
\end{equation}
where $\mathcal{L}_{\text{MSE}}$ represents the weight reconstruction loss and $\mathcal{L}_{\text{c}}$ is a contrastive loss.  
The reconstruction term encourages the decoder to accurately reproduce the original model weights, expressed layer-wise as:
\begin{equation}
\mathcal{L}_{\text{MSE}} = \frac{1}{MN} \sum_{i=1}^{M} \sum_{l=1}^{L} \| \hat{w}_i^{(l)} - w_i^{(l)} \|_2^2,
\label{eq:weight_recon_loss}
\end{equation}
where $w_i^{(l)}$ and $\hat{w}_i^{(l)}$ are the original and reconstructed weights for the $l$-th layer of the $i$-th model in the collection.  

The contrastive component $\mathcal{L}_{\text{c}}$ introduces additional structure in the embedding space by applying data augmentations during training—such as weight permutation (which leverages inherent symmetries in neural networks) and random erasing—to ensure that semantically similar models are mapped closer together while dissimilar ones are pushed apart. In this work, the contrastive term follows the \textit{NT-Xent} loss formulation (see Section~\ref{sec:contrastive}), which provides a more stable and expressive contrastive objective for modelling relationships in weight space.


This combination of reconstruction and contrastive learning encourages the encoder to capture not only numerical similarity in the weights but also functional and architectural relationships between models. The result is a structured embedding space in which proximity correlates with similarity in performance or behaviour.

The learned embedding space enables both discriminative and generative applications. The encoder can be used to extract informative embeddings that summarise a model's functional behaviour, useful for clustering, transfer learning, or model retrieval. Meanwhile, the decoder can generate entirely new sets of weights from latent vectors—supporting generative applications such as zero-shot model synthesis or interpolation between models. This dual capability makes the autoencoder framework particularly appealing for weight space learning, as it provides both interpretability and controllability.

\begin{figure}[!t]
    \centering
    \includegraphics[width=0.65\linewidth]{sane.png}
    \caption[Sequential Autoencoder for Neural Embeddings (SANE) generating a cloud of embeddings for a model weight ~\cite{pmlr-v235-schurholt24a}]{Sequential Autoencoder for Neural Embeddings generating a cloud of embeddings for a model weight ~\cite{pmlr-v235-schurholt24a}}
    \label{fig:sane}
\end{figure}

A major challenge in encoding neural network weights lies in the enormous number of parameters, which quickly becomes infeasible to process directly. \cite{pmlr-v235-schurholt24a} introduced the Sequential Autoencoder for Neural Embeddings (SANE) shown in Figure~\ref{fig:sane}, which addresses this scalability problem by tokenising model weights rather than treating the entire parameter set as a single input.  

Instead of encoding full weight tensors, SANE partitions them into smaller, semantically meaningful \textit{tokens}—for example, by layer or even within layers—and encodes these sequentially. Each token is represented as a point in embedding space, and the entire model is represented as a \textit{cloud of latent tokens} rather than a single vector. This design assumes that sufficient information about the model's global behaviour is preserved through these local token representations, an assumption supported by empirical results in \cite{pmlr-v235-schurholt24a}.

SANE achieves state-of-the-art performance in both discriminative and generative weight-space applications, demonstrating that this token-based decomposition maintains the essential structure of models while dramatically improving scalability. Furthermore, because the approach is sequential, it naturally accommodates heterogeneous architectures—normalising across different layer types and sizes—and allows the generation of new architectures by decoding variable-length token sequences.  