@inproceedings{
      bedionita2025instructionguidedautoregressiveneuralnetwork,
      title={Instruction-Guided Autoregressive Neural Network Parameter Generation},
      author={Bedionita Soro and Bruno Andreis and Song Chong and Sung Ju Hwang},
      booktitle={Workshop on Neural Network Weights as a New Data Modality},
      year={2025},
      url={https://openreview.net/forum?id=QutFK34ea1}
}
@misc{salama2024datasetsizerecoverylora,
      title={Dataset Size Recovery from LoRA Weights}, 
      author={Mohammad Salama and Jonathan Kahana and Eliahu Horwitz and Yedid Hoshen},
      year={2024},
      eprint={2406.19395},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2406.19395}, 
}
@inproceedings{
      meynent2025structureenoughleveragingbehavior,
      title={Structure Is Not Enough: Leveraging Behavior for Neural Network Weight Reconstruction},
      author={L{\'e}o Meynent and Ivan Melev and Konstantin Sch{\"u}rholt and Goeran Kauermann and Damian Borth},
      booktitle={Workshop on Neural Network Weights as a New Data Modality},
      year={2025},
      url={https://openreview.net/forum?id=APsHrpqO3W}
      }
@inproceedings{NEURIPS2022_b2c4b7d3,
 author = {Sch\"{u}rholt, Konstantin and Knyazev, Boris and Gir\'{o}-i-Nieto, Xavier and Borth, Damian},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Koyejo and S. Mohamed and A. Agarwal and D. Belgrave and K. Cho and A. Oh},
 pages = {27906--27920},
 publisher = {Curran Associates, Inc.},
 title = {Hyper-Representations as Generative Models: Sampling Unseen Neural Network Weights},
 url = {https://proceedings.neurips.cc/paper_files/paper/2022/file/b2c4b7d34b3d96b9dc12f7bce424b7ae-Paper-Conference.pdf},
 volume = {35},
 year = {2022}
}
@inproceedings{pmlr-v235-schurholt24a,
    title={Towards Scalable and Versatile Weight Space Learning},
    author={Konstantin Sch{\"u}rholt and Michael W. Mahoney and Damian Borth},
    booktitle={Proceedings of the 41st International Conference on Machine Learning (ICML)},
    year={2024},
    organization={PMLR}
}

@misc{Dean2017BlackBox,
author = {Dean, Jeff},
title = {Keynote Speech on the Future of AI},
booktitle = {Google I/O Conference},
year = {2017},
note = {Statement widely attributed to the keynote speech, highlighting the challenge of interpretability in deep learning systems.},
howpublished = {\url{https://www.google.com/search?q=https://events.google.com/io/}}
}

@misc{huggingface2024review,
  title = {{Open-source AI: Year in Review 2024}},
  author = {{Hugging Face}},
  year = {2024},
  howpublished = {\url{https://huggingface.co/spaces/huggingface/open-source-ai-year-in-review-2024}},
  note = {Accessed: 14 October 2025}
}
@inproceedings{
      schurholt2022modelzoosdatasetdiverse,
      title={Model Zoos: A Dataset of Diverse Populations of Neural Network Models},
      author={Konstantin Sch{\"u}rholt and Diyar Taskiran and Boris Knyazev and Xavier Gir{\'o}-i-Nieto and Damian Borth},
      booktitle={Thirty-sixth Conference on Neural Information Processing Systems Datasets and Benchmarks Track},
      year={2022},
      url={https://openreview.net/forum?id=MOCZI3h8Ye}
}

@misc{unterthiner2021predictingneuralnetworkaccuracy,
      title={Predicting Neural Network Accuracy from Weights}, 
      author={Thomas Unterthiner and Daniel Keysers and Sylvain Gelly and Olivier Bousquet and Ilya Tolstikhin},
      year={2021},
      eprint={2002.11448},
      archivePrefix={arXiv},
      primaryClass={stat.ML},
      url={https://arxiv.org/abs/2002.11448}, 
}

@misc{schurholt2022hyperrepresentationsgenerativemodelssampling,
      title={Hyper-Representations as Generative Models: Sampling Unseen Neural Network Weights}, 
      author={Konstantin Schürholt and Boris Knyazev and Xavier Giró-i-Nieto and Damian Borth},
      year={2022},
      eprint={2209.14733},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2209.14733}, 
}

@misc{agren2022ntxentlossupperbound,
      title={The NT-Xent loss upper bound}, 
      author={Wilhelm Ågren},
      year={2022},
      eprint={2205.03169},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2205.03169}, 
}

@InProceedings{pmlr-v119-chen20j,
  title = {A Simple Framework for Contrastive Learning of Visual Representations},
  author = {Chen, Ting and Kornblith, Simon and Norouzi, Mohammad and Hinton, Geoffrey},
  booktitle = {Proceedings of the 37th International Conference on Machine Learning},
  pages = {1597--1607},
  year = {2020},
  editor = {Daumé III, Hal and Singh, Aarti},
  volume = {119},
  series = {Proceedings of Machine Learning Research},
  month = {13--18 Jul},
  publisher = {PMLR},
  pdf = {https://proceedings.mlr.press/v119/chen20j/chen20j.pdf},
  url = {https://proceedings.mlr.press/v119/chen20j.html}
}

@book{foundationsCVbook,
  title={Foundations of Computer Vision},
  author={Torralba, A. and Isola, P. and Freeman, W.T.},
  isbn={9780262378666},
  lccn={2023024589},
  series={Adaptive Computation and Machine Learning series},
  url={https://mitpress.mit.edu/9780262048972/foundations-of-computer-vision/},
  year={2024},
  publisher={MIT Press}
}

@InProceedings{pmlr-v139-radford21a,
  title = 	 {Learning Transferable Visual Models From Natural Language Supervision},
  author =       {Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and Krueger, Gretchen and Sutskever, Ilya},
  booktitle = 	 {Proceedings of the 38th International Conference on Machine Learning},
  pages = 	 {8748--8763},
  year = 	 {2021},
  editor = 	 {Meila, Marina and Zhang, Tong},
  volume = 	 {139},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {18--24 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v139/radford21a/radford21a.pdf},
  url = 	 {https://proceedings.mlr.press/v139/radford21a.html},
  abstract = 	 {State-of-the-art computer vision systems are trained to predict a fixed set of predetermined object categories. This restricted form of supervision limits their generality and usability since additional labeled data is needed to specify any other visual concept. Learning directly from raw text about images is a promising alternative which leverages a much broader source of supervision. We demonstrate that the simple pre-training task of predicting which caption goes with which image is an efficient and scalable way to learn SOTA image representations from scratch on a dataset of 400 million (image, text) pairs collected from the internet. After pre-training, natural language is used to reference learned visual concepts (or describe new ones) enabling zero-shot transfer of the model to downstream tasks. We study the performance of this approach by benchmarking on over 30 different existing computer vision datasets, spanning tasks such as OCR, action recognition in videos, geo-localization, and many types of fine-grained object classification. The model transfers non-trivially to most tasks and is often competitive with a fully supervised baseline without the need for any dataset specific training. For instance, we match the accuracy of the original ResNet-50 on ImageNet zero-shot without needing to use any of the 1.28 million training examples it was trained on.}
}


@CONFERENCE{ImageNet_VSS09,
        AUTHOR = {Deng, J. and Li, K. and Do, M. and Su, H. and Fei-Fei, L.},        
        TITLE = {{Construction and Analysis of a Large Scale Image Ontology}},
        ORGANIZATION = {Vision Sciences Society},
        YEAR = {2009},
        BIBSOURCE = "http://www.image-net.org/papers/ImageNet_VSS2009.bib"}


@article{He2015DeepRL,
  title={Deep Residual Learning for Image Recognition},
  author={Kaiming He and X. Zhang and Shaoqing Ren and Jian Sun},
  journal={2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  year={2015},
  pages={770-778},
  url={https://api.semanticscholar.org/CorpusID:206594692}
}

@article{Hinton2006Reducing,
  author = {Hinton, Geoffrey E. and Salakhutdinov, Ruslan R.},
  title = {Reducing the Dimensionality of Data with Neural Networks},
  journal = {Science},
  volume = {313},
  number = {5786},
  pages = {504--507},
  year = {2006},
  doi = {10.1126/science.1127647},
  url = {https://www.cs.toronto.edu/~hinton/absps/science.pdf}
}

@misc{wang2020datasetdistillation,
      title={Dataset Distillation}, 
      author={Tongzhou Wang and Jun-Yan Zhu and Antonio Torralba and Alexei A. Efros},
      year={2020},
      eprint={1811.10959},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1811.10959}, 
}


@InProceedings{pmlr-v38-choromanska15,
  title = 	 {{The Loss Surfaces of Multilayer Networks}},
  author = 	 {Choromanska, Anna and Henaff, MIkael and Mathieu, Michael and Ben Arous, Gerard and LeCun, Yann},
  booktitle = 	 {Proceedings of the Eighteenth International Conference on Artificial Intelligence and Statistics},
  pages = 	 {192--204},
  year = 	 {2015},
  editor = 	 {Lebanon, Guy and Vishwanathan, S. V. N.},
  volume = 	 {38},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {San Diego, California, USA},
  month = 	 {09--12 May},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v38/choromanska15.pdf},
  url = 	 {https://proceedings.mlr.press/v38/choromanska15.html},
  abstract = 	 {We study the connection between the highly non-convex loss function of a simple model of the fully-connected feed-forward neural network and the Hamiltonian of the spherical spin-glass model under the assumptions of: i) variable independence, ii) redundancy in network parametrization, and iii) uniformity. These assumptions enable us to explain the complexity of the fully decoupled neural network through the prism of the results from random matrix theory. We show that for large-size decoupled networks the lowest critical values of the random loss function form a layered structure and they are located in a well-defined band lower-bounded by the global minimum. The number of local minima outside that band diminishes exponentially with the size of the network. We empirically verify that the mathematical model exhibits similar behavior as the computer simulations, despite the presence of high dependencies in real networks. We conjecture that both simulated annealing and SGD converge to the band of low critical points, and that all critical points found there are local minima of high quality measured by the test error. This emphasizes a major difference between large- and small-size networks where for the latter poor quality local minima have non-zero probability of being recovered. Finally, we prove that recovering the global minimum becomes harder as the network size increases and that it is in practice irrelevant as global minimum often leads to overfitting.}
}
